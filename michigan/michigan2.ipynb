{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from random import shuffle\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file = 'traindata.txt'):\n",
    "    with open('/home/farzad/Documents/kaggle/'+file, 'r') as f:\n",
    "        labels = []\n",
    "        text = []\n",
    "        \n",
    "        lines = f.readlines()\n",
    "    shuffle(lines)\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        if len(data) == 2:\n",
    "            labels.append(int(data[0]))\n",
    "            text.append(data[1].rstrip())\n",
    "    return text,labels\n",
    "    \n",
    "x_train_text , y_train = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man i loved brokeback mountain! \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "data_text = x_train_text\n",
    "idx = 5\n",
    "print(x_train_text[idx],'\\n',y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  a-zA-Z0-9 حذف تمامی کاراکترهای غیر از"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process(txt):\n",
    "    out = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    out = out.split()\n",
    "    out = [word.lower() for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(thresh = 5):\n",
    "    \n",
    "    count  = dict()\n",
    "    idx = 1\n",
    "    word_index = dict()\n",
    "    for txt in data_text:\n",
    "        words = process(txt)\n",
    "        for word in words:\n",
    "            if word in count.keys():\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word]  = 1\n",
    "    ## بدست اوردن تعداد تکرار هر کلمه در کل متن\n",
    "    \n",
    "    \n",
    "    most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
    "    \n",
    "    for word in most_counts:\n",
    "        word_index[word] = idx\n",
    "        idx+=1\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = dict()\n",
    "idx = 1\n",
    "word_index = dict()\n",
    "for txt in data_text:\n",
    "    words = process(txt)\n",
    "    for word in words:\n",
    "        if word in count.keys():\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word]  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'jane',\n",
       " 'kids',\n",
       " 'horrible',\n",
       " 'apparently',\n",
       " 'theme',\n",
       " 'havent',\n",
       " 'stupid',\n",
       " 'else',\n",
       " 'when',\n",
       " 'than',\n",
       " 'having',\n",
       " 'with',\n",
       " 'school',\n",
       " 'bonkers',\n",
       " 'before',\n",
       " 'dash',\n",
       " 'da',\n",
       " 'it',\n",
       " 'made',\n",
       " 'quite',\n",
       " 'amazing',\n",
       " 'how',\n",
       " 'god',\n",
       " 'show',\n",
       " 'him',\n",
       " 'books',\n",
       " 'want',\n",
       " 'well',\n",
       " 'course',\n",
       " 'way',\n",
       " 'movie',\n",
       " 'picture',\n",
       " 'friends',\n",
       " 'about',\n",
       " 'freakin',\n",
       " 'leah',\n",
       " 'my',\n",
       " 'same',\n",
       " 'totally',\n",
       " 'xmen',\n",
       " 'talk',\n",
       " 'fun',\n",
       " 'type',\n",
       " 'hips',\n",
       " 'anyone',\n",
       " 'liked',\n",
       " 'which',\n",
       " 'anyway',\n",
       " 'right',\n",
       " 'enjoyed',\n",
       " 'looks',\n",
       " 'main',\n",
       " 'throat',\n",
       " 'get',\n",
       " 'actually',\n",
       " 'despised',\n",
       " 'while',\n",
       " 'character',\n",
       " 'vinci',\n",
       " 'retarted',\n",
       " 'better',\n",
       " 'saw',\n",
       " 'all',\n",
       " 'stories',\n",
       " 'hey',\n",
       " 'gay',\n",
       " 'harry',\n",
       " 'these',\n",
       " 'one',\n",
       " 'finished',\n",
       " 'fandom',\n",
       " 'long',\n",
       " 'absolutely',\n",
       " 'me',\n",
       " 'no',\n",
       " 'says',\n",
       " 'things',\n",
       " 'they',\n",
       " 'on',\n",
       " 'such',\n",
       " 'whos',\n",
       " 'lost',\n",
       " 'mtv',\n",
       " 'that',\n",
       " 'gin',\n",
       " 'aching',\n",
       " 'hat',\n",
       " 'hella',\n",
       " 'hung',\n",
       " 'try',\n",
       " 'too',\n",
       " 'officially',\n",
       " 'keys',\n",
       " 'sick',\n",
       " 'malfoy',\n",
       " 'trousers',\n",
       " 'thats',\n",
       " 'went',\n",
       " 'mom',\n",
       " 'theater',\n",
       " 'make',\n",
       " 'told',\n",
       " 'felicia',\n",
       " 'knows',\n",
       " 'sad',\n",
       " 'be',\n",
       " 'there',\n",
       " 'friday',\n",
       " 'also',\n",
       " 'wrong',\n",
       " 'kelsie',\n",
       " 'why',\n",
       " 'rings',\n",
       " 'watch',\n",
       " 'some',\n",
       " 'going',\n",
       " 'talking',\n",
       " 'joining',\n",
       " 'hoot',\n",
       " 'and',\n",
       " 'been',\n",
       " 'opinion',\n",
       " 'cool',\n",
       " 'needs',\n",
       " 'times',\n",
       " 'hates',\n",
       " 'becoming',\n",
       " 'demons',\n",
       " 'book',\n",
       " 'theres',\n",
       " 'movies',\n",
       " 'impossible',\n",
       " 'stand',\n",
       " 'little',\n",
       " 'boring',\n",
       " 'has',\n",
       " 'bitch',\n",
       " 'of',\n",
       " 'laughed',\n",
       " 'dragged',\n",
       " 'are',\n",
       " 'never',\n",
       " 'real',\n",
       " 'insanely',\n",
       " 'suicides',\n",
       " 'yeah',\n",
       " 'homosexuality',\n",
       " 'even',\n",
       " 'crap',\n",
       " 'can',\n",
       " 'grabs',\n",
       " 'start',\n",
       " 'tye',\n",
       " 'jokes',\n",
       " 'see',\n",
       " 'this',\n",
       " 'won',\n",
       " 'again',\n",
       " 'daniel',\n",
       " 'gary',\n",
       " 'ever',\n",
       " 'as',\n",
       " 'lovethe',\n",
       " 'turned',\n",
       " 'news',\n",
       " 'go',\n",
       " 'snuck',\n",
       " 'who',\n",
       " 'had',\n",
       " 'world',\n",
       " 'cowboy',\n",
       " 'money',\n",
       " 'blonds',\n",
       " 'hear',\n",
       " 'kirsten',\n",
       " 'am',\n",
       " 'sucked',\n",
       " 'tell',\n",
       " 'mountain',\n",
       " 'know',\n",
       " 'differently',\n",
       " 'helped',\n",
       " 'series',\n",
       " 'miss',\n",
       " 'side',\n",
       " 'so',\n",
       " 'care',\n",
       " 'reminded',\n",
       " 'novel',\n",
       " '2',\n",
       " 'because',\n",
       " 'through',\n",
       " 'gonna',\n",
       " 'year',\n",
       " 'story',\n",
       " 'thing',\n",
       " 'three',\n",
       " 'past',\n",
       " 'dont',\n",
       " 'seen',\n",
       " 'terrible',\n",
       " 'personally',\n",
       " 's',\n",
       " 'serious',\n",
       " 'likeyeah',\n",
       " 'at',\n",
       " 'lol',\n",
       " 'fact',\n",
       " 'read',\n",
       " 'doesnt',\n",
       " 'night',\n",
       " 'i',\n",
       " 'we',\n",
       " 'until',\n",
       " 'may',\n",
       " 'reading',\n",
       " 'their',\n",
       " 'best',\n",
       " 'only',\n",
       " 'great',\n",
       " 'were',\n",
       " 'fan',\n",
       " 'days',\n",
       " 'second',\n",
       " 'depressing',\n",
       " 'wotshisface',\n",
       " 'interesting',\n",
       " 'anything',\n",
       " 'didnt',\n",
       " 'feel',\n",
       " 'by',\n",
       " 'place',\n",
       " 'might',\n",
       " 'table',\n",
       " 'plain',\n",
       " 'profound',\n",
       " 'watched',\n",
       " 'left',\n",
       " 'good',\n",
       " 'she',\n",
       " 'his',\n",
       " 'have',\n",
       " 'rocks',\n",
       " 'sucks',\n",
       " 'head',\n",
       " 'mean',\n",
       " 'kinda',\n",
       " 'goin',\n",
       " 'combining',\n",
       " 'used',\n",
       " 'community',\n",
       " 'kind',\n",
       " 'cock',\n",
       " 'to',\n",
       " 'suck',\n",
       " 'or',\n",
       " 'letting',\n",
       " 'draco',\n",
       " 'time',\n",
       " 'review',\n",
       " 'betterwe',\n",
       " 'sucking',\n",
       " 'would',\n",
       " 'every',\n",
       " 'far',\n",
       " 'bye',\n",
       " 'them',\n",
       " 'balls',\n",
       " 'awesome',\n",
       " 'dads',\n",
       " 'beautiful',\n",
       " 'what',\n",
       " 'sit',\n",
       " 'potter',\n",
       " 'off',\n",
       " 'seeing',\n",
       " 'something',\n",
       " 'then',\n",
       " 'give',\n",
       " 'said',\n",
       " 'acceptable',\n",
       " 'bad',\n",
       " 'but',\n",
       " 'a',\n",
       " 'not',\n",
       " 'ass',\n",
       " 'groaning',\n",
       " 'awards',\n",
       " 'hate',\n",
       " 'guy',\n",
       " 'over',\n",
       " 'must',\n",
       " '3',\n",
       " 'station',\n",
       " 'mission',\n",
       " 'could',\n",
       " 'silent',\n",
       " 'cruise',\n",
       " 'outshines',\n",
       " 'saying',\n",
       " 'freaking',\n",
       " 'hated',\n",
       " 'ive',\n",
       " 'now',\n",
       " 'least',\n",
       " 'whimpering',\n",
       " 'crappy',\n",
       " 'thinking',\n",
       " 'most',\n",
       " 'if',\n",
       " 'bobbypin',\n",
       " 'big',\n",
       " 'is',\n",
       " 'two',\n",
       " 'crash',\n",
       " 'new',\n",
       " 'cleaning',\n",
       " 'first',\n",
       " 'he',\n",
       " 'much',\n",
       " 'acne',\n",
       " 'lot',\n",
       " 'thought',\n",
       " 'where',\n",
       " 'catcher',\n",
       " 'song',\n",
       " 'crazy',\n",
       " 'few',\n",
       " 'desperately',\n",
       " 'just',\n",
       " 'stars',\n",
       " 'reality',\n",
       " '25',\n",
       " 'everyone',\n",
       " 'sentry',\n",
       " 'you',\n",
       " 'tom',\n",
       " 'felicias',\n",
       " 'material',\n",
       " 'escapades',\n",
       " 'black',\n",
       " 'sure',\n",
       " 'evil',\n",
       " 'bogus',\n",
       " 'ill',\n",
       " 'last',\n",
       " 'okay',\n",
       " 'almost',\n",
       " 'code',\n",
       " 'will',\n",
       " 'out',\n",
       " 'doing',\n",
       " 'count',\n",
       " 'people',\n",
       " 'back',\n",
       " 'lubb',\n",
       " 'enjoy',\n",
       " 'either',\n",
       " 'inaccurate',\n",
       " 'dudeee',\n",
       " 'our',\n",
       " 'do',\n",
       " 'hes',\n",
       " 'hill',\n",
       " 'really',\n",
       " 'excellent',\n",
       " 'brokeback',\n",
       " 'likes',\n",
       " 'action',\n",
       " 'vigor',\n",
       " 'iii',\n",
       " 'fucking',\n",
       " 'other',\n",
       " 'pretty',\n",
       " 'end',\n",
       " 'an',\n",
       " 'its',\n",
       " 'glad',\n",
       " 'always',\n",
       " 'already',\n",
       " 'does',\n",
       " 'up',\n",
       " 'zen',\n",
       " 'quiz',\n",
       " 'cant',\n",
       " 'eyre',\n",
       " 'love',\n",
       " 'from',\n",
       " 'still',\n",
       " 'dies',\n",
       " 'in',\n",
       " 'did',\n",
       " 'panting',\n",
       " 'heard',\n",
       " 'very',\n",
       " 'for',\n",
       " 'got',\n",
       " 'youre',\n",
       " 'life',\n",
       " 'shit',\n",
       " 'b',\n",
       " 'ok',\n",
       " 'yet',\n",
       " 'the',\n",
       " 'deep',\n",
       " 'both',\n",
       " 'wait',\n",
       " 'rockhard',\n",
       " 'begin',\n",
       " 'probably',\n",
       " 'was',\n",
       " 'wanted',\n",
       " 'angels',\n",
       " 'im',\n",
       " 'day',\n",
       " 'awful',\n",
       " 'virgin',\n",
       " 'though',\n",
       " 'being',\n",
       " 'kate',\n",
       " 'your',\n",
       " 'watching',\n",
       " 'slap',\n",
       " 'take',\n",
       " 'man',\n",
       " 'more',\n",
       " 'loved',\n",
       " 'lord',\n",
       " 'into',\n",
       " 'luv',\n",
       " 'down',\n",
       " 'those',\n",
       " 'soo',\n",
       " 'id',\n",
       " 'oh',\n",
       " 'us',\n",
       " 'coz',\n",
       " 'say',\n",
       " 'like',\n",
       " 'should',\n",
       " 'cowboys',\n",
       " 'bit',\n",
       " 'worth',\n",
       " 'noises',\n",
       " 'think',\n",
       " 'since',\n",
       " 'film',\n",
       " 'around',\n",
       " 'making',\n",
       " 'person',\n",
       " 'after']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_counts = [word for word in count.keys() if count[word]>=5]\n",
    "most_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in most_counts:\n",
    "    word_index[word] = idx\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 191,\n",
       " '25': 334,\n",
       " '3': 294,\n",
       " 'a': 285,\n",
       " 'about': 35,\n",
       " 'absolutely': 74,\n",
       " 'acceptable': 282,\n",
       " 'aching': 87,\n",
       " 'acne': 322,\n",
       " 'action': 370,\n",
       " 'actually': 56,\n",
       " 'after': 452,\n",
       " 'again': 159,\n",
       " 'all': 64,\n",
       " 'almost': 349,\n",
       " 'already': 381,\n",
       " 'also': 110,\n",
       " 'always': 380,\n",
       " 'am': 177,\n",
       " 'amazing': 22,\n",
       " 'an': 377,\n",
       " 'and': 121,\n",
       " 'angels': 414,\n",
       " 'anyone': 46,\n",
       " 'anything': 229,\n",
       " 'anyway': 49,\n",
       " 'apparently': 5,\n",
       " 'are': 142,\n",
       " 'around': 449,\n",
       " 'as': 163,\n",
       " 'ass': 287,\n",
       " 'at': 207,\n",
       " 'awards': 289,\n",
       " 'awesome': 270,\n",
       " 'awful': 417,\n",
       " 'b': 402,\n",
       " 'back': 356,\n",
       " 'bad': 283,\n",
       " 'balls': 269,\n",
       " 'be': 107,\n",
       " 'beautiful': 272,\n",
       " 'because': 192,\n",
       " 'becoming': 128,\n",
       " 'been': 122,\n",
       " 'before': 16,\n",
       " 'begin': 410,\n",
       " 'being': 420,\n",
       " 'best': 219,\n",
       " 'better': 62,\n",
       " 'betterwe': 262,\n",
       " 'big': 313,\n",
       " 'bit': 443,\n",
       " 'bitch': 138,\n",
       " 'black': 342,\n",
       " 'blonds': 174,\n",
       " 'bobbypin': 312,\n",
       " 'bogus': 345,\n",
       " 'bonkers': 15,\n",
       " 'book': 130,\n",
       " 'books': 27,\n",
       " 'boring': 136,\n",
       " 'both': 407,\n",
       " 'brokeback': 368,\n",
       " 'but': 284,\n",
       " 'by': 232,\n",
       " 'bye': 267,\n",
       " 'can': 151,\n",
       " 'cant': 386,\n",
       " 'care': 188,\n",
       " 'catcher': 326,\n",
       " 'character': 59,\n",
       " 'cleaning': 318,\n",
       " 'cock': 254,\n",
       " 'code': 350,\n",
       " 'combining': 250,\n",
       " 'community': 252,\n",
       " 'cool': 124,\n",
       " 'could': 297,\n",
       " 'count': 354,\n",
       " 'course': 30,\n",
       " 'cowboy': 172,\n",
       " 'cowboys': 442,\n",
       " 'coz': 438,\n",
       " 'crap': 150,\n",
       " 'crappy': 308,\n",
       " 'crash': 316,\n",
       " 'crazy': 328,\n",
       " 'cruise': 299,\n",
       " 'da': 18,\n",
       " 'dads': 271,\n",
       " 'daniel': 160,\n",
       " 'dash': 17,\n",
       " 'day': 416,\n",
       " 'days': 224,\n",
       " 'deep': 406,\n",
       " 'demons': 129,\n",
       " 'depressing': 226,\n",
       " 'desperately': 330,\n",
       " 'despised': 57,\n",
       " 'did': 393,\n",
       " 'didnt': 230,\n",
       " 'dies': 391,\n",
       " 'differently': 182,\n",
       " 'do': 363,\n",
       " 'does': 382,\n",
       " 'doesnt': 211,\n",
       " 'doing': 353,\n",
       " 'dont': 200,\n",
       " 'down': 432,\n",
       " 'draco': 259,\n",
       " 'dragged': 141,\n",
       " 'dudeee': 361,\n",
       " 'either': 359,\n",
       " 'else': 9,\n",
       " 'end': 376,\n",
       " 'enjoy': 358,\n",
       " 'enjoyed': 51,\n",
       " 'escapades': 341,\n",
       " 'even': 149,\n",
       " 'ever': 162,\n",
       " 'every': 265,\n",
       " 'everyone': 335,\n",
       " 'evil': 344,\n",
       " 'excellent': 367,\n",
       " 'eyre': 387,\n",
       " 'fact': 209,\n",
       " 'fan': 223,\n",
       " 'fandom': 72,\n",
       " 'far': 266,\n",
       " 'feel': 231,\n",
       " 'felicia': 104,\n",
       " 'felicias': 339,\n",
       " 'few': 329,\n",
       " 'film': 448,\n",
       " 'finished': 71,\n",
       " 'first': 319,\n",
       " 'for': 397,\n",
       " 'freakin': 36,\n",
       " 'freaking': 302,\n",
       " 'friday': 109,\n",
       " 'friends': 34,\n",
       " 'from': 389,\n",
       " 'fucking': 373,\n",
       " 'fun': 43,\n",
       " 'gary': 161,\n",
       " 'gay': 67,\n",
       " 'get': 55,\n",
       " 'gin': 86,\n",
       " 'give': 280,\n",
       " 'glad': 379,\n",
       " 'go': 167,\n",
       " 'god': 24,\n",
       " 'goin': 249,\n",
       " 'going': 117,\n",
       " 'gonna': 194,\n",
       " 'good': 240,\n",
       " 'got': 398,\n",
       " 'grabs': 152,\n",
       " 'great': 221,\n",
       " 'groaning': 288,\n",
       " 'guy': 291,\n",
       " 'had': 170,\n",
       " 'harry': 68,\n",
       " 'has': 137,\n",
       " 'hat': 88,\n",
       " 'hate': 290,\n",
       " 'hated': 303,\n",
       " 'hates': 127,\n",
       " 'have': 243,\n",
       " 'havent': 7,\n",
       " 'having': 12,\n",
       " 'he': 320,\n",
       " 'head': 246,\n",
       " 'hear': 175,\n",
       " 'heard': 395,\n",
       " 'hella': 89,\n",
       " 'helped': 183,\n",
       " 'here': 1,\n",
       " 'hes': 364,\n",
       " 'hey': 66,\n",
       " 'hill': 365,\n",
       " 'him': 26,\n",
       " 'hips': 45,\n",
       " 'his': 242,\n",
       " 'homosexuality': 148,\n",
       " 'hoot': 120,\n",
       " 'horrible': 4,\n",
       " 'how': 23,\n",
       " 'hung': 90,\n",
       " 'i': 213,\n",
       " 'id': 435,\n",
       " 'if': 311,\n",
       " 'iii': 372,\n",
       " 'ill': 346,\n",
       " 'im': 415,\n",
       " 'impossible': 133,\n",
       " 'in': 392,\n",
       " 'inaccurate': 360,\n",
       " 'insanely': 145,\n",
       " 'interesting': 228,\n",
       " 'into': 430,\n",
       " 'is': 314,\n",
       " 'it': 19,\n",
       " 'its': 378,\n",
       " 'ive': 304,\n",
       " 'jane': 2,\n",
       " 'joining': 119,\n",
       " 'jokes': 155,\n",
       " 'just': 331,\n",
       " 'kate': 421,\n",
       " 'kelsie': 112,\n",
       " 'keys': 94,\n",
       " 'kids': 3,\n",
       " 'kind': 253,\n",
       " 'kinda': 248,\n",
       " 'kirsten': 176,\n",
       " 'know': 181,\n",
       " 'knows': 105,\n",
       " 'last': 347,\n",
       " 'laughed': 140,\n",
       " 'leah': 37,\n",
       " 'least': 306,\n",
       " 'left': 239,\n",
       " 'letting': 258,\n",
       " 'life': 400,\n",
       " 'like': 440,\n",
       " 'liked': 47,\n",
       " 'likes': 369,\n",
       " 'likeyeah': 206,\n",
       " 'little': 135,\n",
       " 'lol': 208,\n",
       " 'long': 73,\n",
       " 'looks': 52,\n",
       " 'lord': 429,\n",
       " 'lost': 83,\n",
       " 'lot': 323,\n",
       " 'love': 388,\n",
       " 'loved': 428,\n",
       " 'lovethe': 164,\n",
       " 'lubb': 357,\n",
       " 'luv': 431,\n",
       " 'made': 20,\n",
       " 'main': 53,\n",
       " 'make': 102,\n",
       " 'making': 450,\n",
       " 'malfoy': 96,\n",
       " 'man': 426,\n",
       " 'material': 340,\n",
       " 'may': 216,\n",
       " 'me': 75,\n",
       " 'mean': 247,\n",
       " 'might': 234,\n",
       " 'miss': 185,\n",
       " 'mission': 296,\n",
       " 'mom': 100,\n",
       " 'money': 173,\n",
       " 'more': 427,\n",
       " 'most': 310,\n",
       " 'mountain': 180,\n",
       " 'movie': 32,\n",
       " 'movies': 132,\n",
       " 'mtv': 84,\n",
       " 'much': 321,\n",
       " 'must': 293,\n",
       " 'my': 38,\n",
       " 'needs': 125,\n",
       " 'never': 143,\n",
       " 'new': 317,\n",
       " 'news': 166,\n",
       " 'night': 212,\n",
       " 'no': 76,\n",
       " 'noises': 445,\n",
       " 'not': 286,\n",
       " 'novel': 190,\n",
       " 'now': 305,\n",
       " 'of': 139,\n",
       " 'off': 276,\n",
       " 'officially': 93,\n",
       " 'oh': 436,\n",
       " 'ok': 403,\n",
       " 'okay': 348,\n",
       " 'on': 80,\n",
       " 'one': 70,\n",
       " 'only': 220,\n",
       " 'opinion': 123,\n",
       " 'or': 257,\n",
       " 'other': 374,\n",
       " 'our': 362,\n",
       " 'out': 352,\n",
       " 'outshines': 300,\n",
       " 'over': 292,\n",
       " 'panting': 394,\n",
       " 'past': 199,\n",
       " 'people': 355,\n",
       " 'person': 451,\n",
       " 'personally': 203,\n",
       " 'picture': 33,\n",
       " 'place': 233,\n",
       " 'plain': 236,\n",
       " 'potter': 275,\n",
       " 'pretty': 375,\n",
       " 'probably': 411,\n",
       " 'profound': 237,\n",
       " 'quite': 21,\n",
       " 'quiz': 385,\n",
       " 'read': 210,\n",
       " 'reading': 217,\n",
       " 'real': 144,\n",
       " 'reality': 333,\n",
       " 'really': 366,\n",
       " 'reminded': 189,\n",
       " 'retarted': 61,\n",
       " 'review': 261,\n",
       " 'right': 50,\n",
       " 'rings': 114,\n",
       " 'rockhard': 409,\n",
       " 'rocks': 244,\n",
       " 's': 204,\n",
       " 'sad': 106,\n",
       " 'said': 281,\n",
       " 'same': 39,\n",
       " 'saw': 63,\n",
       " 'say': 439,\n",
       " 'saying': 301,\n",
       " 'says': 77,\n",
       " 'school': 14,\n",
       " 'second': 225,\n",
       " 'see': 156,\n",
       " 'seeing': 277,\n",
       " 'seen': 201,\n",
       " 'sentry': 336,\n",
       " 'series': 184,\n",
       " 'serious': 205,\n",
       " 'she': 241,\n",
       " 'shit': 401,\n",
       " 'should': 441,\n",
       " 'show': 25,\n",
       " 'sick': 95,\n",
       " 'side': 186,\n",
       " 'silent': 298,\n",
       " 'since': 447,\n",
       " 'sit': 274,\n",
       " 'slap': 424,\n",
       " 'snuck': 168,\n",
       " 'so': 187,\n",
       " 'some': 116,\n",
       " 'something': 278,\n",
       " 'song': 327,\n",
       " 'soo': 434,\n",
       " 'stand': 134,\n",
       " 'stars': 332,\n",
       " 'start': 153,\n",
       " 'station': 295,\n",
       " 'still': 390,\n",
       " 'stories': 65,\n",
       " 'story': 196,\n",
       " 'stupid': 8,\n",
       " 'such': 81,\n",
       " 'suck': 256,\n",
       " 'sucked': 178,\n",
       " 'sucking': 263,\n",
       " 'sucks': 245,\n",
       " 'suicides': 146,\n",
       " 'sure': 343,\n",
       " 'table': 235,\n",
       " 'take': 425,\n",
       " 'talk': 42,\n",
       " 'talking': 118,\n",
       " 'tell': 179,\n",
       " 'terrible': 202,\n",
       " 'than': 11,\n",
       " 'that': 85,\n",
       " 'thats': 98,\n",
       " 'the': 405,\n",
       " 'theater': 101,\n",
       " 'their': 218,\n",
       " 'them': 268,\n",
       " 'theme': 6,\n",
       " 'then': 279,\n",
       " 'there': 108,\n",
       " 'theres': 131,\n",
       " 'these': 69,\n",
       " 'they': 79,\n",
       " 'thing': 197,\n",
       " 'things': 78,\n",
       " 'think': 446,\n",
       " 'thinking': 309,\n",
       " 'this': 157,\n",
       " 'those': 433,\n",
       " 'though': 419,\n",
       " 'thought': 324,\n",
       " 'three': 198,\n",
       " 'throat': 54,\n",
       " 'through': 193,\n",
       " 'time': 260,\n",
       " 'times': 126,\n",
       " 'to': 255,\n",
       " 'told': 103,\n",
       " 'tom': 338,\n",
       " 'too': 92,\n",
       " 'totally': 40,\n",
       " 'trousers': 97,\n",
       " 'try': 91,\n",
       " 'turned': 165,\n",
       " 'two': 315,\n",
       " 'tye': 154,\n",
       " 'type': 44,\n",
       " 'until': 215,\n",
       " 'up': 383,\n",
       " 'us': 437,\n",
       " 'used': 251,\n",
       " 'very': 396,\n",
       " 'vigor': 371,\n",
       " 'vinci': 60,\n",
       " 'virgin': 418,\n",
       " 'wait': 408,\n",
       " 'want': 28,\n",
       " 'wanted': 413,\n",
       " 'was': 412,\n",
       " 'watch': 115,\n",
       " 'watched': 238,\n",
       " 'watching': 423,\n",
       " 'way': 31,\n",
       " 'we': 214,\n",
       " 'well': 29,\n",
       " 'went': 99,\n",
       " 'were': 222,\n",
       " 'what': 273,\n",
       " 'when': 10,\n",
       " 'where': 325,\n",
       " 'which': 48,\n",
       " 'while': 58,\n",
       " 'whimpering': 307,\n",
       " 'who': 169,\n",
       " 'whos': 82,\n",
       " 'why': 113,\n",
       " 'will': 351,\n",
       " 'with': 13,\n",
       " 'won': 158,\n",
       " 'world': 171,\n",
       " 'worth': 444,\n",
       " 'wotshisface': 227,\n",
       " 'would': 264,\n",
       " 'wrong': 111,\n",
       " 'xmen': 41,\n",
       " 'yeah': 147,\n",
       " 'year': 195,\n",
       " 'yet': 404,\n",
       " 'you': 337,\n",
       " 'your': 422,\n",
       " 'youre': 399,\n",
       " 'zen': 384}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dictionary  452\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "word_index = tokenize()\n",
    "num_words = len(word_index)\n",
    "print('length of the dictionary ',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## بیشترین طول جمله\n",
    "\n",
    "def getMax(data):\n",
    "    max_tokens = 0 \n",
    "    for txt in data:\n",
    "        if max_tokens < len(txt.split()):\n",
    "            max_tokens = len(txt.split())\n",
    "    return max_tokens\n",
    "max_tokens = getMax(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data):\n",
    "    tokens = []\n",
    "    for txt in data:\n",
    "        words = process(txt)\n",
    "        seq = [0] * max_tokens\n",
    "        i = 0 \n",
    "        \n",
    "        for word in words:\n",
    "            start = max_tokens-len(words)  \n",
    "            if word.lower() in word_index.keys():\n",
    "                seq[i+start] = word_index[word]\n",
    "            i+=1\n",
    "        tokens.append(seq)        \n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0 32 86]]\n"
     ]
    }
   ],
   "source": [
    "print(create_sequences(['awesome movie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 279, 304,  11,\n",
       "       188])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens = create_sequences(x_train_text)\n",
    "x_train_tokens[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens[7085].shape\n",
    "# type(x_train_tokens)\n",
    "# np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 4,961\n",
      "Trainable params: 4,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "\n",
    "\n",
    "model = Sequential() # Creating Model By Adding Layers One By One\n",
    "\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(40, activation = \"relu\", input_shape=(40, )))\n",
    "\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(40, activation = \"relu\"))\n",
    "          \n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(40, activation = \"relu\"))\n",
    "\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6377 samples, validate on 709 samples\n",
      "Epoch 1/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.3039 - acc: 0.8827 - val_loss: 0.1898 - val_acc: 0.9351\n",
      "Epoch 2/10\n",
      "6377/6377 [==============================] - 1s 105us/step - loss: 0.3091 - acc: 0.8822 - val_loss: 0.1943 - val_acc: 0.9351\n",
      "Epoch 3/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.2907 - acc: 0.8890 - val_loss: 0.1981 - val_acc: 0.9365\n",
      "Epoch 4/10\n",
      "6377/6377 [==============================] - 1s 131us/step - loss: 0.2994 - acc: 0.8929 - val_loss: 0.2104 - val_acc: 0.9267\n",
      "Epoch 5/10\n",
      "6377/6377 [==============================] - 1s 105us/step - loss: 0.2713 - acc: 0.8982 - val_loss: 0.1893 - val_acc: 0.9323\n",
      "Epoch 6/10\n",
      "6377/6377 [==============================] - 1s 103us/step - loss: 0.2693 - acc: 0.9017 - val_loss: 0.1913 - val_acc: 0.9323\n",
      "Epoch 7/10\n",
      "6377/6377 [==============================] - 1s 101us/step - loss: 0.2643 - acc: 0.9034 - val_loss: 0.1816 - val_acc: 0.9337\n",
      "Epoch 8/10\n",
      "6377/6377 [==============================] - 1s 99us/step - loss: 0.2666 - acc: 0.9012 - val_loss: 0.1897 - val_acc: 0.9351\n",
      "Epoch 9/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.2451 - acc: 0.9094 - val_loss: 0.1905 - val_acc: 0.9323\n",
      "Epoch 10/10\n",
      "6377/6377 [==============================] - 1s 104us/step - loss: 0.2552 - acc: 0.9054 - val_loss: 0.1892 - val_acc: 0.9309\n",
      "Test-Accuracy: 0.9330042313117065\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(\n",
    " x_train_tokens, np.array(y_train),\n",
    " epochs= 10,\n",
    " batch_size = 16,\n",
    " validation_split=.1\n",
    ")\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6731 samples, validate on 355 samples\n",
      "Epoch 1/5\n",
      "6731/6731 [==============================] - 0s 63us/step - loss: 0.2271 - acc: 0.9171 - val_loss: 0.1804 - val_acc: 0.9380\n",
      "Epoch 2/5\n",
      "6731/6731 [==============================] - 0s 56us/step - loss: 0.2365 - acc: 0.9135 - val_loss: 0.1716 - val_acc: 0.9352\n",
      "Epoch 3/5\n",
      "6731/6731 [==============================] - 0s 57us/step - loss: 0.2339 - acc: 0.9120 - val_loss: 0.1615 - val_acc: 0.9465\n",
      "Epoch 4/5\n",
      "6731/6731 [==============================] - 0s 55us/step - loss: 0.2296 - acc: 0.9132 - val_loss: 0.1685 - val_acc: 0.9268\n",
      "Epoch 5/5\n",
      "6731/6731 [==============================] - 0s 69us/step - loss: 0.2241 - acc: 0.9137 - val_loss: 0.1678 - val_acc: 0.9324\n",
      "Test-Accuracy: 0.9357746478873239\n"
     ]
    }
   ],
   "source": [
    "res2 = model.fit(np.array(x_train_tokens),np.array(y_train),validation_split=0.05, epochs=5, batch_size=32)\n",
    "print(\"Test-Accuracy:\", np.mean(res2.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7086/7086 [==============================] - 0s 38us/step\n",
      "Test score: 0.13834074508023403\n",
      "Test accuracy: 0.9458086366980486\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = load_dataset()\n",
    "x_test_tokens = create_sequences(x_test)\n",
    "\n",
    "#x_test_tokens\n",
    "\n",
    "score, acc = model.evaluate(x_test_tokens, y_test,batch_size=16)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
