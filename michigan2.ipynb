{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file = 'traindata.txt'):\n",
    "    with open('/home/farzad/Documents/kaggle/'+file, 'r') as f:\n",
    "        labels = []\n",
    "        text = []\n",
    "        \n",
    "        lines = f.readlines()\n",
    "    shuffle(lines)\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        if len(data) == 2:\n",
    "            labels.append(int(data[0]))\n",
    "            text.append(data[1].rstrip())\n",
    "    return text,labels\n",
    "    \n",
    "x_train_text , y_train = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to start reading the Harry Potter series again because that is one awesome story. \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "data_text = x_train_text\n",
    "idx = 5\n",
    "print(x_train_text[idx],'\\n',y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process(txt):\n",
    "    out = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    out = out.split()\n",
    "    out = [word.lower() for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(thresh = 5):\n",
    "    count  = dict()\n",
    "    idx = 1\n",
    "    word_index = dict()\n",
    "    for txt in data_text:\n",
    "        words = process(txt)\n",
    "        for word in words:\n",
    "            if word in count.keys():\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word]  = 1\n",
    "    most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
    "    for word in most_counts:\n",
    "        word_index[word] = idx\n",
    "        idx+=1\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dictionary  452\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "word_index = tokenize()\n",
    "num_words = len(word_index)\n",
    "print('length of the dictionary ',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMax(data):\n",
    "    max_tokens = 0 \n",
    "    for txt in data:\n",
    "        if max_tokens < len(txt.split()):\n",
    "            max_tokens = len(txt.split())\n",
    "    return max_tokens\n",
    "max_tokens = getMax(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data):\n",
    "    tokens = []\n",
    "    for txt in data:\n",
    "        words = process(txt)\n",
    "        seq = [0] * max_tokens\n",
    "        i = 0 \n",
    "        for word in words:\n",
    "            start = max_tokens-len(words)\n",
    "            if word.lower() in word_index.keys():\n",
    "                seq[i+start] = word_index[word]\n",
    "            i+=1\n",
    "        tokens.append(seq)        \n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0 32 86]]\n"
     ]
    }
   ],
   "source": [
    "print(create_sequences(['awesome movie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 279, 304,  11,\n",
       "       188])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens = create_sequences(x_train_text)\n",
    "x_train_tokens[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# embedding_size = 8\n",
    "# model.add(Embedding(input_dim=num_words,\n",
    "#                     output_dim=embedding_size,\n",
    "#                     input_length=max_tokens,\n",
    "#                     name='layer_embedding'))\n",
    "\n",
    "# model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n",
    "# model.add(GRU(units=8, name = \"gru_2\" ,return_sequences=True))\n",
    "# model.add(GRU(units=4, name= \"gru_3\"))\n",
    "# model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
    "# optimizer = Adam(lr=1e-3)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=optimizer,\n",
    "#               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens[7085].shape\n",
    "# type(x_train_tokens)\n",
    "# np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 4,961\n",
      "Trainable params: 4,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "\n",
    "\n",
    "model = Sequential() # Creating Model By Adding Layers One By One\n",
    "\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(40, activation = \"relu\", input_shape=(40, )))\n",
    "\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(40, activation = \"relu\"))\n",
    "          \n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(40, activation = \"relu\"))\n",
    "\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6377 samples, validate on 709 samples\n",
      "Epoch 1/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.3039 - acc: 0.8827 - val_loss: 0.1898 - val_acc: 0.9351\n",
      "Epoch 2/10\n",
      "6377/6377 [==============================] - 1s 105us/step - loss: 0.3091 - acc: 0.8822 - val_loss: 0.1943 - val_acc: 0.9351\n",
      "Epoch 3/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.2907 - acc: 0.8890 - val_loss: 0.1981 - val_acc: 0.9365\n",
      "Epoch 4/10\n",
      "6377/6377 [==============================] - 1s 131us/step - loss: 0.2994 - acc: 0.8929 - val_loss: 0.2104 - val_acc: 0.9267\n",
      "Epoch 5/10\n",
      "6377/6377 [==============================] - 1s 105us/step - loss: 0.2713 - acc: 0.8982 - val_loss: 0.1893 - val_acc: 0.9323\n",
      "Epoch 6/10\n",
      "6377/6377 [==============================] - 1s 103us/step - loss: 0.2693 - acc: 0.9017 - val_loss: 0.1913 - val_acc: 0.9323\n",
      "Epoch 7/10\n",
      "6377/6377 [==============================] - 1s 101us/step - loss: 0.2643 - acc: 0.9034 - val_loss: 0.1816 - val_acc: 0.9337\n",
      "Epoch 8/10\n",
      "6377/6377 [==============================] - 1s 99us/step - loss: 0.2666 - acc: 0.9012 - val_loss: 0.1897 - val_acc: 0.9351\n",
      "Epoch 9/10\n",
      "6377/6377 [==============================] - 1s 106us/step - loss: 0.2451 - acc: 0.9094 - val_loss: 0.1905 - val_acc: 0.9323\n",
      "Epoch 10/10\n",
      "6377/6377 [==============================] - 1s 104us/step - loss: 0.2552 - acc: 0.9054 - val_loss: 0.1892 - val_acc: 0.9309\n",
      "Test-Accuracy: 0.9330042313117065\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(\n",
    " x_train_tokens, np.array(y_train),\n",
    " epochs= 10,\n",
    " batch_size = 16,\n",
    " validation_split=.1\n",
    ")\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6731 samples, validate on 355 samples\n",
      "Epoch 1/5\n",
      "6731/6731 [==============================] - 0s 63us/step - loss: 0.2271 - acc: 0.9171 - val_loss: 0.1804 - val_acc: 0.9380\n",
      "Epoch 2/5\n",
      "6731/6731 [==============================] - 0s 56us/step - loss: 0.2365 - acc: 0.9135 - val_loss: 0.1716 - val_acc: 0.9352\n",
      "Epoch 3/5\n",
      "6731/6731 [==============================] - 0s 57us/step - loss: 0.2339 - acc: 0.9120 - val_loss: 0.1615 - val_acc: 0.9465\n",
      "Epoch 4/5\n",
      "6731/6731 [==============================] - 0s 55us/step - loss: 0.2296 - acc: 0.9132 - val_loss: 0.1685 - val_acc: 0.9268\n",
      "Epoch 5/5\n",
      "6731/6731 [==============================] - 0s 69us/step - loss: 0.2241 - acc: 0.9137 - val_loss: 0.1678 - val_acc: 0.9324\n",
      "Test-Accuracy: 0.9357746478873239\n"
     ]
    }
   ],
   "source": [
    "res2 = model.fit(np.array(x_train_tokens),np.array(y_train),validation_split=0.05, epochs=5, batch_size=32)\n",
    "print(\"Test-Accuracy:\", np.mean(res2.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7086/7086 [==============================] - 0s 38us/step\n",
      "Test score: 0.13834074508023403\n",
      "Test accuracy: 0.9458086366980486\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = load_dataset()\n",
    "x_test_tokens = create_sequences(x_test)\n",
    "\n",
    "#x_test_tokens\n",
    "\n",
    "score, acc = model.evaluate(x_test_tokens, y_test,batch_size=16)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
